import streamlit as st
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase
import av
import numpy as np
from io import BytesIO
import tempfile
import soundfile as sf
import torch
import torchaudio
import librosa

from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCTC, AutoFeatureExtractor, Wav2Vec2ForCTC

st.set_page_config(
    page_title="Hindi Audio Transcription & Analysis (No OpenAI)",
    page_icon="üéôÔ∏è",
    layout="centered",
    initial_sidebar_state="expanded",
)

# Cache model loading for performance
@st.cache_resource(show_spinner=True)
def load_asr_model():
    # Using facebook wav2vec2 large for Hindi ASR
    # Alternatively use "openai/whisper-small" (Hugging Face tokenizer + model)
    # But here we pick "facebook/wav2vec2-large-xlsr-53-hindi"
    model_name = "facebook/wav2vec2-large-xlsr-53-hindi"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = Wav2Vec2ForCTC.from_pretrained(model_name)
    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)
    return tokenizer, model, feature_extractor

@st.cache_resource(show_spinner=True)
def load_translation_model():
    # Helsinki-NLP opus-mt-hi-en for Hindi to English translation
    model_name = "Helsinki-NLP/opus-mt-hi-en"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    return tokenizer, model

@st.cache_resource(show_spinner=True)
def load_summarization_model():
    # Use a multilingual summarization model
    # e.g. "mrm8488/mbart-large-finetuned-summarize-news" or facebook/mbart-large-50-many-to-many-mmt
    model_name = "mrm8488/mbart-large-finetuned-summarize-news"
    summarizer = pipeline("summarization", model=model_name, truncation=True, min_length=30, max_length=130)
    return summarizer

@st.cache_resource(show_spinner=True)
def load_sentiment_model():
    # Multilingual sentiment analysis model
    model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
    sentiment = pipeline("sentiment-analysis", model=model_name)
    return sentiment

# Audio Recorder Using streamlit-webrtc, saves wav audio frames in memory
class AudioRecorder(AudioProcessorBase):
    def __init__(self) -> None:
        self.frames = []

    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        self.frames.append(frame)
        return frame

def audio_frames_to_wav_bytes(frames):
    # Convert collected audio frames to bytes wav format
    if not frames:
        return None
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        container = av.open(tmp.name, mode='w')
        stream = container.add_stream('pcm_s16le', rate=48000)
        stream.layout = "mono"
        for frame in frames:
            # Make frame compatible: mono, 48000Hz
            frame.pts = None
            packet = stream.encode(frame)
            if packet:
                container.mux(packet)
        # Flush
        packet = stream.encode(None)
        if packet:
            container.mux(packet)
        container.close()
        tmp.seek(0)
        with open(tmp.name, "rb") as f:
            data = f.read()
    return data

def speech_file_to_array(audio_bytes):
    # We convert the audio bytes to numpy array and sample rate
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp:
        tmp.write(audio_bytes)
        tmp.flush()
        speech, sr = librosa.load(tmp.name, sr=16000, mono=True)
    return speech, sr

def transcribe(audio_bytes, tokenizer, model, feature_extractor):
    # Convert audio bytes to numpy array at 16kHz
    speech, sr = speech_file_to_array(audio_bytes)
    # Feature extraction
    input_values = feature_extractor(speech, sampling_rate=sr, return_tensors="pt").input_values
    # Forward pass
    with torch.no_grad():
        logits = model(input_values).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = tokenizer.decode(predicted_ids[0])
    return transcription.lower()

def translate_hi_to_en(text, tokenizer, model):
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)
    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translated_text

def main():
    st.title("üéôÔ∏è Hindi Audio Transcription, Translation, Summary & Sentiment Analysis (No OpenAI)")

    st.markdown(
        """
        This app allows you to either **record your audio (Hindi or Hinglish)** or **upload an audio file**.
        After processing, it generates the **Hindi transcript**, its **English translation**, 
        a **summary**, and performs **sentiment analysis**.

        ---
        """
    )

    # Load models once
    with st.spinner("Loading models... This happens once at startup"):
        tokenizer_asr, model_asr, feature_extractor = load_asr_model()
        tokenizer_trans, model_trans = load_translation_model()
        summarizer = load_summarization_model()
        sentiment_analyzer = load_sentiment_model()

    # Mode selection
    input_mode = st.radio("Choose Input Mode", ["Record Audio üé§", "Upload Audio üìÅ"])

    uploaded_audio = None

    if input_mode == "Record Audio üé§":
        st.write("Click **Start** to record your Hindi/Hinglish audio. Click **Stop** when finished then click **Get Recorded Audio** below.")
        ctx = webrtc_streamer(
            key="audio_recorder",
            mode="sendrecv",
            media_stream_constraints={"audio": True, "video": False},
            audio_processor_factory=AudioRecorder,
            async_processing=True,
        )

        if st.button("Get Recorded Audio"):
            if ctx.audio_processor and ctx.audio_processor.frames:
                audio_bytes = audio_frames_to_wav_bytes(ctx.audio_processor.frames)
                if audio_bytes:
                    uploaded_audio = audio_bytes
                else:
                    st.warning("No audio recorded, please record something.")
            else:
                st.warning("No audio recorded yet. Please record audio first.")
    else:
        uploaded_file = st.file_uploader(
            "Upload your audio file",
            type=["wav", "mp3", "m4a", "flac", "ogg", "webm", "aac"],
        )
        if uploaded_file:
            uploaded_audio = uploaded_file.read()

    if uploaded_audio:
        with st.spinner("Performing transcription..."):
            try:
                hindi_text = transcribe(uploaded_audio, tokenizer_asr, model_asr, feature_extractor)
            except Exception as e:
                st.error(f"Error during transcription: {e}")
                return
        st.subheader("üìù Hindi Transcript")
        st.text_area("Hindi Transcript", value=hindi_text, height=150)

        with st.spinner("Translating to English..."):
            try:
                english_text = translate_hi_to_en(hindi_text, tokenizer_trans, model_trans)
            except Exception as e:
                st.error(f"Error during translation: {e}")
                return
        st.subheader("üåê English Translation")
        st.text_area("English Translation", value=english_text, height=150)

        with st.spinner("Generating summary..."):
            try:
                summary_result = summarizer(english_text, max_length=130, min_length=30, do_sample=False)
                summary_text = summary_result[0]['summary_text']
            except Exception as e:
                st.error(f"Error during summarization: {e}")
                return
        st.subheader("üì∞ Summary")
        st.text_area("Summary", value=summary_text, height=150)

        with st.spinner("Performing sentiment analysis..."):
            try:
                sentiment_result = sentiment_analyzer(summary_text)
                # sentiment_result is a list of dicts [{label: 'POSITIVE', score:0.9},...]
                sentiment_text = f"{sentiment_result[0]['label']} (score: {sentiment_result[0]['score']:.2f})"
            except Exception as e:
                st.error(f"Error during sentiment analysis: {e}")
                return
        st.subheader("üòä Sentiment Analysis")
        st.markdown(f"**Sentiment:** {sentiment_text}")

    else:
        st.info("Please record or upload an audio file to start processing.")

    st.markdown("---")
    st.markdown(
        """
        _Note: This app uses Hugging Face transformers for speech recognition, translation, summarization, and sentiment analysis._  
        _Models run locally, which may be slow without a GPU._  
        _Audio recording works best in Chrome desktop._  
        _Supported audio formats: wav, mp3, m4a, flac, ogg, webm, aac._
        """
    )

if __name__ == "__main__":
    main()
</content>
</create_file>

<attempt_completion>
‚úÖ Task complete: Created a Streamlit app that allows users to record or upload Hindi/Hinglish audio, then transcribes it using a Hugging Face wav2vec2 Hindi model, translates Hindi transcript to English using Helsinki-NLP model, generates a summary, and performs sentiment analysis‚Äîall without using OpenAI API.

- Uses streamlit-webrtc for audio recording.
- Uses transformers pipelines and pretrained models.
- Entirely self-contained in a single Python file.
- Instructions and UI for easy usage.
- Ready for deployment on Streamlit Cloud or local machine.

To run the app locally:

1. Install dependencies:
